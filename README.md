<a id="table-of-contents"></a>

## üìö Table of Contents
- [ü§ñSQL-IPDC (Shared Q-Learning + Iterative PD)](#sql-ipdc)
  - [1Ô∏è‚É£ Method](#sql-method)
  - [2Ô∏è‚É£ Getting Started](#sql-getting-started)
  - [3Ô∏è‚É£ Repository Structure](#sql-structure)
  - [4Ô∏è‚É£ Instructions](#sql-instructions)
- [ü§ñMPC](#mpc-panda)
  - [1Ô∏è‚É£ Method](#mpc-method)
  - [2Ô∏è‚É£ Getting Started](#mpc-getting-started)
  - [3Ô∏è‚É£ Repository Structure](#mpc-structure)
  - [4Ô∏è‚É£ Instructions](#mpc-instructions)

----

# ü§ñ Shared Q-Learning<br>Based Iterative Controller Design
# 1Ô∏è‚É£ Method
This repository integrates ***multi-agent shared Q-Learning*** for grid-based path planning with an offline, ***iterative PD gain tuning*** scheme for accurate path tracking by multiple Franka Emika Panda manipulators in ***PyBullet***.
Agents periodically share the best-performing Q-table to accelerate learning; on the control side, PD gains (Kp, Kd) are tuned offline using RMSE as the performance metric. The framework decouples learning (planning) from control (tracking), which eliminates online optimization during control runtime and simplifies deployment.
****
reference
# 2Ô∏è‚É£ Getting Started
## Required Packages
- Python 3.10.12
- Python packages:
  
  ‚úÖ numpy

  ‚úÖ matplotlib

  ‚úÖ pybullet 3.2.7
 
## üîß Installation
```bash
git clone https://github.com/DSCL-CHO/Shared_Q-Learning_Based_Iterative_Controller_Design.git
cd Shared_Q-Learning_Based_Iterative_Controller_Design/SQL-IPDC
```
## üîß Install Python dependencies (e.g., via pip)
#### Quick
```bash
pip install --upgrade pip
pip install numpy pybullet pybullet_data matplotlib
```
#### Recommended (virtual environment)
```bash
python3 -m venv .venv
source .venv/bin/activate      

pip install --upgrade pip
pip install numpy pybullet pybullet_data matplotlib
```
# 3Ô∏è‚É£ Repository Structure
- SQL-IPDC/
  - common/
      - \_\_init\_\_.py
      - gridworld_render.py
      - gridworld.py
      - utils.py
  - q_learning/
      - \_\_init\_\_.py
      - train_q_learning.py
      - extract_path.py
  - kp_then_kd/
      - 01_tune_kp.py
      - 02_tune_kd.py
      - coord_to_xyz.py
      - panda_env.py
  - kd_then_kp/
      - 01_tune_kd.py
      - 02_tune_kp.py
      - coord_to_xyz.py
      - panda_env.py
- README.md
# 4Ô∏è‚É£ Instructions
Below is a step-by-step guide to each folder and how to run the corresponding scripts.
## 4.1 common/
**`gridworld.py`**  ‚Äî Defines the reward function.
## 4.2 q_learning/
**`train_q_learning.py`**  ‚Äî Runs ***shared Q-learning*** across multiple agents.

Produces/plots each agent‚Äôs cumulative reward curves (and logs).
```bash
cd q_learning
python train_q_learning.py
```
**`extract_path.py `** ‚Äî Extracts the optimal path from the learned Q-table and prints/exports the result.

```bash
cd q_learning
python extract_path.py
```
## 4.3 kp_then_kd/
**`01_tune_kp.py`** ‚Äî Fix ***Kd*** and ***tune Kp first*** based on tracking error metrics (e.g., RMSE).
**`02_tune_kd.py`** ‚Äî Using the best Kp, ***tune Kd*** to further reduce the error.

```bash
cd kp_then_kd
python 01_tune_kp.py
python 02_tune_kd.py
```
## 4.4 kd_then_kp/
**`01_tune_kd.py`** ‚Äî Fix ***Kp*** and ***tune Kd first*** based on tracking error metrics (e.g., RMSE).
**`02_tune_kp.py`** ‚Äî Using the best Kd, ***tune Kp*** to further reduce the error.

```bash
cd kd_then_kp/
python 01_tune_kd.py
python 02_tune_kp.py
```
****
# ü§ñ MPC Panda
# 1Ô∏è‚É£ Method
The proposed project implements joint-space **model predictive control (MPC)** for the 7-DoF Franka Emika Panda manipulator. The plant is modeled in continuous time, and a discrete-time prediction model is obtained by integrating the dynamics with a **fourth-order Runge‚ÄìKutta (RK4)**. At each sampling instant, the controller solves a finite-horizon optimal control problem to compute joint torques that track a prescribed sequence of reference joint angles while satisfying joint-position and joint-velocity constraints (with optional torque limits). The optimization is formulated in ***CasADi*** and solved with ***IPOPT***. Simulations demonstrate accurate tracking of both constant and time-varying references under these constraints.
****
reference
# 2Ô∏è‚É£ Getting Started
## Required Packages
- Python 3.10.12
- Python packages:

  ‚úÖ matplotlib
  
  ‚úÖ numpy

  ‚úÖ CasADi 3.6.3 (https://web.casadi.org/get/)

## üîß Installation
```bash
git clone https://github.com/DSCL-CHO/Shared_Q-Learning_Based_Iterative_Controller_Design.git
cd Shared_Q-Learning_Based_Iterative_Controller_Design/MPC
```
## üîß Install Python dependencies (e.g., via pip)
### Quick
```bash
pip install numpy matplotlib "casadi==3.6.3"
```
### Recommended (virtual environment)
```bash
python3 -m venv .venv
source .venv/bin/activate     
pip install --upgrade pip
pip install numpy matplotlib "casadi==3.6.3"
```
# 3Ô∏è‚É£ Repository Structure
- MPC_Panda/
  - MPC/
      - main.py
      - plant.py
      - get_M.py
      - get_C.py
      - get_G.py   
- README.md
# 4Ô∏è‚É£ Instructions
Below is a step-by-step guide to each folder and how to run the corresponding scripts.

**`main.py`**  ‚Äî Run MPC simulation.
- ‚è±Ô∏è `sim_T` ‚Äî total simulation time
- ‚è≥ `sim_dt` ‚Äî simulation step
- ‚è≤Ô∏è `mpc_dt` ‚Äî MPC control step
- üî≠ `N_horizon` ‚Äî prediction horizon

```bash
# Run
python main_panda.py
```
